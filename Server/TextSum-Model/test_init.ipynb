{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "path=\"./model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This library implements the generic methods of the model library PreTrainedModel.\n"
     ]
    }
   ],
   "source": [
    "myInput=(\n",
    "    \"The bare PEGASUS Model outputting raw hidden-states without any specific head on top. This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\"\n",
    ")\n",
    "input=tokenizer(myInput,max_length=1024,return_tensors=\"pt\",truncation=True)\n",
    "output = model.generate(\n",
    "    input[\"input_ids\"],\n",
    "    max_length=100,\n",
    "    min_length=10\n",
    "    )\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6, 'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "# path = \"./model\"\n",
    "# tokenizer.save_pretrained(path)\n",
    "# model.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This library implements the generic methods of the model library PreTrainedModel.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "\n",
    "myInput=(\n",
    "    \"The bare PEGASUS Model outputting raw hidden-states without any specific head on top. This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\"\n",
    ")\n",
    "input=tokenizer(myInput,max_length=1024,return_tensors=\"pt\",truncation=True)\n",
    "output = model.generate(\n",
    "    input[\"input_ids\"],\n",
    "    max_length=100,\n",
    "    min_length=10\n",
    "    )\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
