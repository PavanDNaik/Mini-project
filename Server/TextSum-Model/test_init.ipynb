{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "path=\"./model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This library implements the generic methods of the model library PreTrainedModel.\n"
          ]
        }
      ],
      "source": [
        "myInput=(\n",
        "    \"The bare PEGASUS Model outputting raw hidden-states without any specific head on top. This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\"\n",
        ")\n",
        "input=tokenizer(myInput,max_length=1024,return_tensors=\"pt\",truncation=True)\n",
        "output = model.generate(\n",
        "    input[\"input_ids\"],\n",
        "    max_length=100,\n",
        "    min_length=10\n",
        "    )\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(path)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(path)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# path = \"./model\"\n",
        "# tokenizer.save_pretrained(path)\n",
        "# model.save_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This library implements the generic methods of the superclass PreTrainedModel, which implements the methods of the pre-trained model, pruning heads, and embeddings, as well as the methods of the pre-trained model, pruning heads, and embeddings, which implements the methods of the pre-trained model, pruning heads, and embeddings, as well as the methods of the pre-trained model, pruning heads, and embeddings, which implements the methods of the pre-trained model\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "path=\"./model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
        "\n",
        "myInput=(\n",
        "    \"The bare PEGASUS Model outputting raw hidden-states without any specific head on top. This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\"\n",
        ")\n",
        "input=tokenizer(myInput,max_length=1024,return_tensors=\"pt\",truncation=True)\n",
        "output = model.generate(\n",
        "    input[\"input_ids\"],\n",
        "    max_length=100,\n",
        "    min_length=50\n",
        "    )\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM , TextStreamer\n",
        "\n",
        "path=\"./model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:452: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0.6` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> This library implements a generic superclass for the model PRETrained, which implements a number of methods</s>\n",
            "<transformers.generation.streamers.TextStreamer object at 0x00000166909D1150>\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
        "\n",
        "streamer = TextStreamer(tokenizer=tokenizer,skip_prompt=True)\n",
        "\n",
        "myInput=(\n",
        "    \"The bare PEGASUS Model outputting raw hidden-states without any specific head on top. This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\"\n",
        ")\n",
        "input=tokenizer(myInput,max_length=1024,return_tensors=\"pt\",truncation=True)\n",
        "output = model.generate(\n",
        "    **input,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=20,\n",
        "    num_beams=1\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM , TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "path=\"./model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:452: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0.6` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "This \n",
            "library \n",
            "implements \n",
            "a \n",
            "generic \n",
            "\n",
            "superclass \n",
            "for \n",
            "the \n",
            "model \n",
            "\n",
            "\n",
            "PRETrained, \n",
            "which \n",
            "implements \n",
            "a \n",
            "number \n",
            "of \n",
            "\n",
            "methods</s>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "streamer = TextIteratorStreamer(tokenizer=tokenizer,skip_prompt=True)\n",
        "\n",
        "myInput=(\n",
        "    \"The bare PEGASUS Model outputting raw hidden-states without any specific head on top. This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\"\n",
        ")\n",
        "\n",
        "input=tokenizer(myInput,max_length=1024,return_tensors=\"pt\",truncation=True)\n",
        "\n",
        "generation_kwargs = dict(input, streamer=streamer, max_new_tokens=20,num_beams=1)\n",
        "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "for new_text in streamer:\n",
        "    print(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:1\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
